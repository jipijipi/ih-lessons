{"cells":[{"cell_type":"markdown","metadata":{"id":"66pqqERbR_TC"},"source":["## Ensemble Methods"]},{"cell_type":"markdown","metadata":{"id":"bNwm83KER_TF"},"source":["In our continuous quest to enhance the accuracy and robustness of our predictive models for California housing prices, we delve into the realm of ensemble methods. Ensemble methods, renowned for their capability to combine multiple models to achieve superior predictive performance, offer a promising avenue for refining our housing price predictions."]},{"cell_type":"markdown","metadata":{"id":"cQFKrSS2R_TF"},"source":["#### Loading and preparing the data"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WQl3rX7NR_TF"},"outputs":[],"source":["from sklearn.datasets import  fetch_california_housing\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.ensemble import BaggingRegressor, RandomForestRegressor,AdaBoostRegressor, GradientBoostingRegressor\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDLWGDmER_TG","outputId":"08f3e5b4-d48a-4f27-d30f-57a173d1561a"},"outputs":[],"source":["california = fetch_california_housing()\n","print(california[\"DESCR\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EkEWAOqR_TH","outputId":"30c9b23f-6ebd-448d-dc8d-8dc2e9960034"},"outputs":[],"source":["df_cali = pd.DataFrame(california[\"data\"], columns = california[\"feature_names\"])\n","df_cali[\"median_house_value\"] = california[\"target\"]\n","\n","df_cali.head()"]},{"cell_type":"markdown","metadata":{"id":"at4JizyDR_TI"},"source":["#### Normalization & Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"z06GELOUR_TI"},"source":["Like we did in Feature Engineering lesson, we are going to normalize our data and select a subset of columns as our features."]},{"cell_type":"markdown","metadata":{"id":"mrVsJNFhR_TH"},"source":["#### Train Test Split"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5UOl_QkoR_TI"},"outputs":[],"source":["features = df_cali.drop(columns = [\"median_house_value\",\"AveOccup\", \"Population\", \"AveBedrms\"])\n","target = df_cali[\"median_house_value\"]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7k5zp9GBR_TI"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"YmBD4IINR_TI"},"source":["Create an instance of the normalizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOd3-i6FR_TI"},"outputs":[],"source":["normalizer = MinMaxScaler()\n","\n","normalizer.fit(X_train)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zTXrQY7UR_TI"},"outputs":[],"source":["X_train_norm = normalizer.transform(X_train)\n","\n","X_test_norm = normalizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIb89YdJR_TJ","outputId":"ab052281-a562-42ac-f8ac-1558075cd69a"},"outputs":[],"source":["X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n","X_train_norm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOiL_XdwR_TJ","outputId":"f752616c-57ca-42ec-e625-c1a910776320"},"outputs":[],"source":["X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)\n","X_test_norm.head()"]},{"cell_type":"markdown","metadata":{"id":"iMZ5ouIsR_TJ"},"source":["## Bagging and Pasting"]},{"cell_type":"markdown","metadata":{},"source":["Bagging involves training multiple instances of the same base model on different subsets of the training data. The final prediction is obtained by averaging or voting over predictions from these models."]},{"cell_type":"markdown","metadata":{"id":"m9MQEw6qR_TJ"},"source":["Just for baseline, our current best model is a Decision Tree with R-Squared of 0.70, lets see how ensembles works"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hrB1f4B2R_TJ"},"outputs":[],"source":["bagging_reg = BaggingRegressor(DecisionTreeRegressor(max_depth=20),\n","                               n_estimators=100,\n","                               max_samples = 1000)"]},{"cell_type":"markdown","metadata":{"id":"14hd5Qq3R_TJ"},"source":["Training Bagging model with our normalized data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuImEavmR_TJ","outputId":"58acd191-3d3d-4af3-de29-cf5bab8852ea"},"outputs":[],"source":["bagging_reg.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{"id":"R9zsbDM8R_TJ"},"source":["Evaluate model's performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDQMWcvJR_TJ","outputId":"9d0745ec-f60e-4780-af35-828050bb5e26"},"outputs":[],"source":["pred = bagging_reg.predict(X_test_norm)\n","\n","print(\"MAE\", mean_absolute_error(pred, y_test))\n","print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n","print(\"R2 score\", bagging_reg.score(X_test_norm, y_test))"]},{"cell_type":"markdown","metadata":{"id":"OZ1nbY9zR_TJ"},"source":["Combining multiple trees, in this case 100, indeed yield a stronger model, now we are at 0.72 R-Squared!\n","\n","Let's explore more!"]},{"cell_type":"markdown","metadata":{"id":"L_pXy5Z4R_TN"},"source":["In Bagging methods, we have many base estimators, so there is no feature importance method implemented."]},{"cell_type":"markdown","metadata":{},"source":["## Random Patches"]},{"cell_type":"markdown","metadata":{},"source":["While in Bagging/Pasting, we randomize the training data that each predictor (estimator) learns from. However, in a Random Patches Method, we go a step further by also **randomizing the features** that each predictor trains with."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a Random Forest"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[],"source":["forest = RandomForestRegressor(n_estimators=100,\n","                             max_depth=20)"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["forest.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = forest.predict(X_test_norm)\n","\n","print(\"MAE\", mean_absolute_error(pred, y_test))\n","print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n","print(\"R2 score\", forest.score(X_test_norm, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["By randomizing data also features that every estimators will learn from, we obtain even a better model!\n","\n","We are now at 0.82 R-Squared."]},{"cell_type":"markdown","metadata":{},"source":["## AdaBoost"]},{"cell_type":"markdown","metadata":{},"source":["Now, instead of training our estimators independently by training them in parallel, each estimators will learn at its predecessor's errors and focus on those datapoints where it failed."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a AdaBoost model"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[],"source":["ada_reg = AdaBoostRegressor(DecisionTreeRegressor(max_depth=20),\n","                            n_estimators=100)"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["ada_reg.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = ada_reg.predict(X_test_norm)\n","\n","print(\"MAE\", mean_absolute_error(pred, y_test))\n","print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n","print(\"R2 score\", ada_reg.score(X_test_norm, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["Even better! By randomizing training set, features and also focusing where the previous estimator failed, we obtained a better model!"]},{"cell_type":"markdown","metadata":{},"source":["## Gradient Boosting"]},{"cell_type":"markdown","metadata":{},"source":["Now, each estimator will predict the error caused by its predecessor."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a AdaBoost model"]},{"cell_type":"code","execution_count":19,"metadata":{},"outputs":[],"source":["gb_reg = GradientBoostingRegressor(max_depth=20,\n","                                   n_estimators=100)"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["gb_reg.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = gb_reg.predict(X_test_norm)\n","\n","print(\"MAE\", mean_absolute_error(pred, y_test))\n","print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n","print(\"R2 score\", gb_reg.score(X_test_norm, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["Gradient Boosting compared with AdaBoosting, really doesnt seems doing a great job.\n","\n","**However, note that none of the hyperparameters of all models we've tried where fine tunned.**\n","\n"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
