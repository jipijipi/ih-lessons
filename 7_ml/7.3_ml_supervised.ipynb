{"cells":[{"cell_type":"markdown","metadata":{"id":"66pqqERbR_TC"},"source":["## Machine Learning Supervised"]},{"cell_type":"markdown","metadata":{"id":"znHecGgkR_TE"},"source":["## Table of Contents"]},{"cell_type":"markdown","metadata":{"id":"bNwm83KER_TF"},"source":["So far, we have focused on using KNN as our model to predict California housing prices. However, there are other models worth exploring. Today, we will experiment with both simple Linear Regression and Decision Trees to understand how they explain our target variable. In machine learning, we typically choose our model based on the relationship between our features and the target variable, or simply by selecting the model with the higher score"]},{"cell_type":"markdown","metadata":{"id":"geU3scpzR_TF"},"source":["Yesterday, we applied some feature engineering techniques, and our model indeed increased its performance. Now, let's see how Linear Regression and Decision Tree perform when we apply the same feature engineering techniques."]},{"cell_type":"markdown","metadata":{"id":"cQFKrSS2R_TF"},"source":["#### Loading and preparing the data"]},{"cell_type":"code","execution_count":1,"metadata":{"id":"WQl3rX7NR_TF"},"outputs":[],"source":["from sklearn.datasets import  fetch_california_housing\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.tree import DecisionTreeRegressor\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler\n","from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDLWGDmER_TG","outputId":"08f3e5b4-d48a-4f27-d30f-57a173d1561a"},"outputs":[],"source":["california = fetch_california_housing()\n","print(california[\"DESCR\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EkEWAOqR_TH","outputId":"30c9b23f-6ebd-448d-dc8d-8dc2e9960034"},"outputs":[],"source":["df_cali = pd.DataFrame(california[\"data\"], columns = california[\"feature_names\"])\n","df_cali[\"median_house_value\"] = california[\"target\"]\n","\n","df_cali.head()"]},{"cell_type":"markdown","metadata":{"id":"at4JizyDR_TI"},"source":["#### Normalization & Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"z06GELOUR_TI"},"source":["Like we did in Feature Engineering lesson, we are going to normalize our data and select a subset of columns as our features."]},{"cell_type":"markdown","metadata":{"id":"mrVsJNFhR_TH"},"source":["#### Train Test Split"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"5UOl_QkoR_TI"},"outputs":[],"source":["features = df_cali.drop(columns = [\"median_house_value\",\"AveOccup\", \"Population\", \"AveBedrms\"])\n","target = df_cali[\"median_house_value\"]"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"7k5zp9GBR_TI"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"YmBD4IINR_TI"},"source":["Create an instance of the normalizer"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pOd3-i6FR_TI"},"outputs":[],"source":["normalizer = MinMaxScaler()\n","\n","normalizer.fit(X_train)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"zTXrQY7UR_TI"},"outputs":[],"source":["X_train_norm = normalizer.transform(X_train)\n","\n","X_test_norm = normalizer.transform(X_test)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIb89YdJR_TJ","outputId":"ab052281-a562-42ac-f8ac-1558075cd69a"},"outputs":[],"source":["X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n","X_train_norm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOiL_XdwR_TJ","outputId":"f752616c-57ca-42ec-e625-c1a910776320"},"outputs":[],"source":["X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)\n","X_test_norm.head()"]},{"cell_type":"markdown","metadata":{"id":"iMZ5ouIsR_TJ"},"source":["## Linear Regression"]},{"cell_type":"markdown","metadata":{"id":"m9MQEw6qR_TJ"},"source":["Let's create an instance of Linear Regression model."]},{"cell_type":"code","execution_count":10,"metadata":{"id":"hrB1f4B2R_TJ"},"outputs":[],"source":["lin_reg = LinearRegression()"]},{"cell_type":"markdown","metadata":{"id":"14hd5Qq3R_TJ"},"source":["Training Linear Regression with our normalized data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuImEavmR_TJ","outputId":"58acd191-3d3d-4af3-de29-cf5bab8852ea"},"outputs":[],"source":["lin_reg.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{"id":"R9zsbDM8R_TJ"},"source":["Evaluate model's performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDQMWcvJR_TJ","outputId":"9d0745ec-f60e-4780-af35-828050bb5e26"},"outputs":[],"source":["pred = lin_reg.predict(X_test_norm)\n","\n","print(\"MAE\", mean_absolute_error(pred, y_test))\n","print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n","print(\"R2 score\", lin_reg.score(X_test_norm, y_test))"]},{"cell_type":"markdown","metadata":{"id":"OZ1nbY9zR_TJ"},"source":["Linear Regression yielding a worse score than our previous model, KNN."]},{"cell_type":"markdown","metadata":{"id":"L_pXy5Z4R_TN"},"source":["In Linear Regression, we often assess feature importance by examining the coefficients in the model. These coefficients indicate the impact of each feature on the model's predictions."]},{"cell_type":"markdown","metadata":{},"source":["- Determine the coefficients (Î²) in the linear regression equation corresponding to each feature.\n","- The magnitude of these coefficients reflects the relative importance of the features. **Greater absolute values suggest more substantial impacts.**"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["lin_reg_coef = {feature : coef for feature, coef in zip(X_train_norm.columns, lin_reg.coef_)}\n","lin_reg_coef"]},{"cell_type":"markdown","metadata":{},"source":["We can conclude that **Median Income** have the highest impact in our model."]},{"cell_type":"markdown","metadata":{},"source":["## Decision Tree"]},{"cell_type":"markdown","metadata":{},"source":["So far between KNN and Liner Regression, the first yield a better score, let's see how a Decision Tree performs."]},{"cell_type":"markdown","metadata":{},"source":["- Initialize a Decision Tree instance\n","\n","- Setting max_depth as 10, this means we will allow our tree to split 10 times"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[],"source":["tree = DecisionTreeRegressor(max_depth=10)"]},{"cell_type":"markdown","metadata":{},"source":["- Training the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tree.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{},"source":["- Evaluate the model"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["X_train_norm"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["y_test"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["pred = tree.predict(X_test_norm)\n","\n","print(\"MAE\", mean_absolute_error(pred, y_test))\n","print(\"RMSE\", mean_squared_error(pred, y_test, squared=False))\n","print(\"R2 score\", tree.score(X_test_norm, y_test))"]},{"cell_type":"markdown","metadata":{},"source":["Often we check what are the most relevant features, like we did before in Linear Regression."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["tree_importance = {feature : importance for feature, importance in zip(X_train_norm.columns, tree.feature_importances_)}\n","tree_importance           "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import export_text\n","\n","tree_viz = export_text(tree, feature_names=list(X_train_norm.columns))\n","print(tree_viz)\n"]},{"cell_type":"markdown","metadata":{},"source":["A bit overwhelming to see, let's use graphviz library.\n","\n","**Note**: you will need to install graphivz - pip install graphviz"]},{"cell_type":"markdown","metadata":{},"source":["- We will train a decision tree, in this case with max_depth=2 to better see the diagram"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.tree import DecisionTreeRegressor, export_graphviz\n","import graphviz\n","\n","tree = DecisionTreeRegressor(max_depth=2)\n","tree.fit(X_train_norm, y_train)\n","\n","\n","dot_data = export_graphviz(tree, out_file=\"tree.dot\", filled=True, rounded=True, feature_names=X_train_norm.columns)\n","\n","with open(\"tree.dot\") as f:\n","    dot_graph = f.read()\n","graphviz.Source(dot_graph)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
