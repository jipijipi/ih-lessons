{"cells":[{"cell_type":"markdown","metadata":{"id":"66pqqERbR_TC"},"source":["## Feature Engineering"]},{"cell_type":"markdown","metadata":{"id":"znHecGgkR_TE"},"source":["## Table of Contents"]},{"cell_type":"markdown","metadata":{"id":"bNwm83KER_TF"},"source":["Feature Engineering is crucial in Machine Learning. It involves selecting, transforming, and creating features from raw data to improve model performance and interpretability. Effective feature engineering can significantly enhance the predictive power and generalization ability of machine learning models."]},{"cell_type":"markdown","metadata":{"id":"geU3scpzR_TF"},"source":["Yesterday, in the KNN Regression aproach, we saw a pretty poor model. Let's apply some feature engineering techniques to see if it improves our model."]},{"cell_type":"markdown","metadata":{"id":"cQFKrSS2R_TF"},"source":["#### Loading and preparing the data"]},{"cell_type":"code","execution_count":56,"metadata":{"id":"WQl3rX7NR_TF"},"outputs":[],"source":["from sklearn.datasets import  fetch_california_housing\n","import pandas as pd\n","import numpy as np\n","\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsRegressor\n","\n","from sklearn.preprocessing import MinMaxScaler, StandardScaler"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XDLWGDmER_TG","outputId":"08f3e5b4-d48a-4f27-d30f-57a173d1561a"},"outputs":[],"source":["california = fetch_california_housing()\n","print(california[\"DESCR\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9EkEWAOqR_TH","outputId":"30c9b23f-6ebd-448d-dc8d-8dc2e9960034"},"outputs":[],"source":["df_cali = pd.DataFrame(california[\"data\"], columns = california[\"feature_names\"])\n","df_cali[\"median_house_value\"] = california[\"target\"]\n","\n","df_cali.head()"]},{"cell_type":"markdown","metadata":{"id":"nEn564aKR_TH"},"source":["#### Checking for anomalies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0VFrlJX3R_TH","outputId":"e08fc803-b08a-47f3-fdde-a4946ca9943f"},"outputs":[],"source":["df_cali.info()"]},{"cell_type":"markdown","metadata":{"id":"mrVsJNFhR_TH"},"source":["#### Train Test Split"]},{"cell_type":"code","execution_count":60,"metadata":{"id":"5UOl_QkoR_TI"},"outputs":[],"source":["features = df_cali.drop(columns = [\"median_house_value\"])\n","target = df_cali[\"median_house_value\"]"]},{"cell_type":"code","execution_count":61,"metadata":{"id":"7k5zp9GBR_TI"},"outputs":[],"source":["X_train, X_test, y_train, y_test = train_test_split(features, target, test_size = 0.20, random_state=0)"]},{"cell_type":"markdown","metadata":{"id":"at4JizyDR_TI"},"source":["#### Normalization"]},{"cell_type":"markdown","metadata":{"id":"z06GELOUR_TI"},"source":["During normalization or standardization, it's essential to fit the model to the training data exclusively, preventing any exposure to the test data to avoid potential data leakage issues."]},{"cell_type":"markdown","metadata":{"id":"YmBD4IINR_TI"},"source":["Create an instance of the normalizer"]},{"cell_type":"code","execution_count":62,"metadata":{"id":"pOd3-i6FR_TI"},"outputs":[],"source":["normalizer = MinMaxScaler()"]},{"cell_type":"markdown","metadata":{"id":"4pUtjfWOR_TI"},"source":["Fit it to our training data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iD3WJ6ZiR_TI","outputId":"aba0bca2-6bfb-4541-d49b-102b7e48be30"},"outputs":[],"source":["normalizer.fit(X_train)"]},{"cell_type":"markdown","metadata":{"id":"x6ihgRt2R_TI"},"source":["Transforming our training and testing data"]},{"cell_type":"code","execution_count":64,"metadata":{"id":"zTXrQY7UR_TI"},"outputs":[],"source":["X_train_norm = normalizer.transform(X_train)\n","\n","X_test_norm = normalizer.transform(X_test)"]},{"cell_type":"markdown","metadata":{"id":"L4xqkOQJR_TI"},"source":["When applying transformations of our dataframe, normalizer will return an array instead of a dataframe object"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f2D6j6ErR_TI","outputId":"f5382b58-26b9-4a3c-feb0-3d6d2e64b8fd"},"outputs":[],"source":["X_train_norm"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iIb89YdJR_TJ","outputId":"ab052281-a562-42ac-f8ac-1558075cd69a"},"outputs":[],"source":["X_train_norm = pd.DataFrame(X_train_norm, columns = X_train.columns)\n","X_train_norm.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BOiL_XdwR_TJ","outputId":"f752616c-57ca-42ec-e625-c1a910776320"},"outputs":[],"source":["X_test_norm = pd.DataFrame(X_test_norm, columns = X_test.columns)\n","X_test_norm.head()"]},{"cell_type":"markdown","metadata":{"id":"iMZ5ouIsR_TJ"},"source":["##### KNN Regressor - modeling"]},{"cell_type":"markdown","metadata":{"id":"m9MQEw6qR_TJ"},"source":["Let's create an instance of KNN with the same hyperparameter as before, n_neighbors = 10."]},{"cell_type":"code","execution_count":68,"metadata":{"id":"hrB1f4B2R_TJ"},"outputs":[],"source":["knn = KNeighborsRegressor(n_neighbors=10)"]},{"cell_type":"markdown","metadata":{"id":"14hd5Qq3R_TJ"},"source":["Training KNN to our normalized data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zuImEavmR_TJ","outputId":"58acd191-3d3d-4af3-de29-cf5bab8852ea"},"outputs":[],"source":["knn.fit(X_train_norm, y_train)"]},{"cell_type":"markdown","metadata":{"id":"R9zsbDM8R_TJ"},"source":["Evaluate model's performance"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oDQMWcvJR_TJ","outputId":"9d0745ec-f60e-4780-af35-828050bb5e26"},"outputs":[],"source":["knn.score(X_test_norm, y_test)"]},{"cell_type":"markdown","metadata":{"id":"OZ1nbY9zR_TJ"},"source":["With raw data we obtain a R2 of 0.16, just by normalizing our data, model's perfomance increase a lot to a R2 of 0.70.\n","\n","This happens because KNN is a distance based algorithm, so its suffers a lot with data in completely different scales."]},{"cell_type":"markdown","metadata":{"id":"kdEDyfxvR_TJ"},"source":["## Feature Selection"]},{"cell_type":"markdown","metadata":{"id":"9cA4MW1xR_TJ"},"source":["Even though normalizing our data had a huge impact on KNN performance, we are currently using every single feature of the dataset.\n","\n","Now let's do a selection of features based on correlactions between themselves but also with the target.\n","\n","We want low correlaction between features, but high correlaction between features and our target."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_p9kostgR_TJ","outputId":"9aa13ecf-1872-4b20-fd1a-f2291c890497"},"outputs":[],"source":["corr=np.abs(df_cali.corr())\n","\n","#Set up mask for triangle representation\n","mask = np.zeros_like(corr, dtype=bool)\n","mask[np.triu_indices_from(mask)] = True\n","\n","# Set up the matplotlib figure\n","f, ax = plt.subplots(figsize=(10, 10))\n","# Generate a custom diverging colormap\n","cmap = sns.diverging_palette(220, 10, as_cmap=True)\n","# Draw the heatmap with the mask and correct aspect ratio\n","sns.heatmap(corr, mask=mask,  vmax=1,square=True, linewidths=.5, cbar_kws={\"shrink\": .5},annot = corr)\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"S1j-1wcPR_TJ"},"source":["By the correlation matrix we can see that:\n","- \"AveRooms\" is highly correlated with \"AveBedrms\", so we drop the one less correlated with our target\n","- \"AveOccup\" and \"Population\" also have pretty low correlation with our target variable, so lets remove them from our selected features"]},{"cell_type":"code","execution_count":72,"metadata":{"id":"kD2M3pasR_TK"},"outputs":[],"source":["X_train_reduced = X_train_norm.drop(columns = [\"AveOccup\", \"Population\", \"AveBedrms\"])\n","X_test_reduced = X_test_norm.drop(columns = [\"AveOccup\", \"Population\", \"AveBedrms\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qelLKY5kR_TN","outputId":"4cf1f5b9-ec8c-428e-8615-c633b7d089f7"},"outputs":[],"source":["knn = KNeighborsRegressor(n_neighbors=10)\n","knn.fit(X_train_reduced, y_train)\n","\n","knn.score(X_test_reduced, y_test)"]},{"cell_type":"markdown","metadata":{"id":"L_pXy5Z4R_TN"},"source":["By normalizing our data and selecting a subset of available features, we were able to massively improve our model, increasing the R2 score from 0.16 to 0.70\n","\n","Notice that we still haven't fine-tuned our hyperparameter, so we will be able to improve even more our model."]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.4"}},"nbformat":4,"nbformat_minor":0}
