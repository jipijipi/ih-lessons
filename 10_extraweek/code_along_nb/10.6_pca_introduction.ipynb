{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e0054bb2",
   "metadata": {},
   "source": [
    "\n",
    "# Principal Component Analysis (PCA)\n",
    "\n",
    "Principal Component Analysis (PCA) is a statistical technique used for dimensionality reduction. It transforms the data into a new coordinate system such that the greatest variance by any projection of the data comes to lie on the first coordinate (called the first principal component), the second greatest variance on the second coordinate, and so on.\n",
    "\n",
    "## Theoretical Background\n",
    "\n",
    "PCA involves the following steps:\n",
    "\n",
    "1. **Standardize the Data**: Center the data by subtracting the mean of each feature and scale to unit variance.\n",
    "2. **Covariance Matrix**: Compute the covariance matrix of the standardized data.\n",
    "3. **Eigenvalues and Eigenvectors**: Calculate the eigenvalues and eigenvectors of the covariance matrix.\n",
    "4. **Principal Components**: Sort the eigenvectors by decreasing eigenvalues and select the top k eigenvectors.\n",
    "5. **Transform Data**: Project the original data onto the selected eigenvectors to get the principal components.\n",
    "\n",
    "Mathematically, PCA can be represented as:\n",
    "\n",
    "$$\n",
    " X_{new} = X \\cdot W \n",
    "$$\n",
    "\n",
    "where X is the original data, W is the matrix of selected eigenvectors, and X_new is the transformed data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fe4ca30",
   "metadata": {},
   "source": [
    "\n",
    "## Hands-on Example\n",
    "\n",
    "Let's walk through a hands-on example using Python and the scikit-learn library.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5a0ce82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd84aff",
   "metadata": {},
   "source": [
    "\n",
    "### Step 1: Load and Standardize the Data\n",
    "\n",
    "We will use the Iris dataset for this example.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9a7478",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55a1718f",
   "metadata": {},
   "source": [
    "\n",
    "### Step 2: Covariance Matrix and Eigenvalues\n",
    "\n",
    "Compute the covariance matrix and find the eigenvalues and eigenvectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18523cc6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05a3b441",
   "metadata": {},
   "source": [
    "\n",
    "### Step 3: PCA with Scikit-learn\n",
    "\n",
    "Perform PCA using the scikit-learn library and transform the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98b5d341",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b12d66a6",
   "metadata": {},
   "source": [
    "\n",
    "### Step 4: Visualize the Result\n",
    "\n",
    "Visualize the first two principal components.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6714cb34",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7180f69c",
   "metadata": {},
   "source": [
    "## Pros and Cons of PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b3e8b9",
   "metadata": {},
   "source": [
    "### Advantages of PCA\n",
    "\n",
    "- **Dimensionality Reduction**:\n",
    "  - PCA reduces the number of features in a dataset while retaining most of the variance (information), making it easier to visualize and analyze high-dimensional data.\n",
    "  - It helps in reducing computational costs, as fewer dimensions mean less computation in subsequent analyses or model training.\n",
    "\n",
    "- **Noise Reduction**:\n",
    "  - By focusing on the principal components that capture the most variance, PCA can filter out noise and irrelevant features, which improves the performance of machine learning models.\n",
    "\n",
    "- **Feature Extraction**:\n",
    "  - PCA generates new features (principal components) that are linear combinations of the original features. These new features can sometimes reveal patterns that were not apparent in the original data.\n",
    "\n",
    "- **Improved Model Performance**:\n",
    "  - Reducing the number of features can help mitigate the risk of overfitting, especially in scenarios with small sample sizes relative to the number of features.\n",
    "  - PCA can improve the accuracy and generalization of machine learning models by removing multicollinearity (highly correlated features).\n",
    "\n",
    "- **Data Visualization**:\n",
    "  - PCA is often used to reduce data to two or three dimensions, making it easier to visualize complex, high-dimensional datasets and identify patterns or clusters.\n",
    "\n",
    "- **Uncorrelated Features**:\n",
    "  - The principal components generated by PCA are uncorrelated, which can be beneficial for some machine learning algorithms that assume feature independence.\n",
    "\n",
    "### Disadvantages of PCA\n",
    "\n",
    "- **Loss of Interpretability**:\n",
    "  - The new features (principal components) created by PCA are linear combinations of the original features, making them difficult to interpret in the context of the original variables.\n",
    "  - It can be challenging to explain the results to stakeholders who may not understand the underlying transformations.\n",
    "\n",
    "- **Assumption of Linearity**:\n",
    "  - PCA assumes that the relationships between features are linear. It may not capture complex, nonlinear relationships in the data, which can limit its effectiveness in some cases.\n",
    "\n",
    "- **Sensitivity to Scaling**:\n",
    "  - PCA is sensitive to the scale of the data. If the features have different units or ranges, they need to be standardized before applying PCA. Failure to do so can lead to misleading results.\n",
    "\n",
    "- **Information Loss**:\n",
    "  - While PCA aims to retain as much variance as possible, some information is inevitably lost when reducing the dimensionality. If too few principal components are selected, important information may be discarded.\n",
    "\n",
    "- **Not Suitable for Categorical Data**:\n",
    "  - PCA is primarily designed for continuous numerical data. Applying PCA to datasets with categorical features can be problematic unless those features are properly encoded.\n",
    "\n",
    "- **Computationally Intensive**:\n",
    "  - For very large datasets, especially those with a high number of features, PCA can be computationally expensive, as it requires the computation of covariance matrices and eigenvectors.\n",
    "\n",
    "- **Assumes Mean-Centered Data**:\n",
    "  - PCA assumes that the data is centered around the origin (mean of zero). If the data is not mean-centered, the results of PCA may be inaccurate.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceeb98b",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756a606b",
   "metadata": {},
   "source": [
    "**PCA helps simplify complex datasets by reducing the number of dimensions, making it easier to visualize and analyze data while retaining the most important information.**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
